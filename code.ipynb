{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP 551 Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMr-DcFlmcB2"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "from scipy import sparse\n",
        "from scipy.sparse import hstack, vstack\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(1234)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crIJMQdCGvW0"
      },
      "source": [
        "# Cleaning Data: 20 News Group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C7iPViYtJRr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673bd981-6c2f-4513-d21b-c0df2293f9b1"
      },
      "source": [
        "# 20newsgroupdataset\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "twenty_train = fetch_20newsgroups(subset='train',remove =['headers','footers','quotes'])\n",
        "\n",
        "count_vect_train = CountVectorizer()\n",
        "X_train_counts = count_vect_train.fit_transform(twenty_train.data)\n",
        "Y_train_counts = twenty_train.target\n",
        "\n",
        "twenty_test = fetch_20newsgroups(subset='test',remove =['headers','footers','quotes'])\n",
        "#count_vect_test = CountVectorizer()\n",
        "X_test_counts = count_vect_train.transform(twenty_test.data)\n",
        "Y_test_counts = twenty_test.target\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH8JZgRSsJJd"
      },
      "source": [
        "#Splitting the training dataset for the 20 news group\n",
        "\n",
        "#20% of the training data\n",
        "X_train20,X_20,Y_train20,Y_20 = train_test_split(X_train_counts, Y_train_counts, train_size = 0.2)\n",
        "\n",
        "#40% of the training data\n",
        "X_train40,X_40,Y_train40,Y_40 = train_test_split(X_train_counts, Y_train_counts, train_size = 0.4)\n",
        "\n",
        "#60% of the training data\n",
        "X_train60,X_60,Y_train60,Y_60 = train_test_split(X_train_counts, Y_train_counts, train_size = 0.6)\n",
        "\n",
        "#80% of the training data\n",
        "X_train80,X_80,Y_train80,Y_80 = train_test_split(X_train_counts, Y_train_counts, train_size = 0.8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWH9CONfHKEf"
      },
      "source": [
        "# Cleaning Data: IMDb Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lkxRIZfyqoc",
        "outputId": "9cf71f2f-98b2-47c6-b8bc-efb55a2f96ba"
      },
      "source": [
        "# IMDB Reviews\n",
        "# Used: https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/',force_remount=True)\n",
        "!ls \"/content/drive/My Drive/COMP551/\"\n",
        "\n",
        "\n",
        "reviews_train = []\n",
        "for line in open('/content/drive/My Drive/COMP551/full_train.txt', 'r'):\n",
        "    reviews_train.append(line.strip())\n",
        "    \n",
        "reviews_test = []\n",
        "for line in open('/content/drive/My Drive/COMP551/full_test.txt', 'r'):\n",
        "    reviews_test.append(line.strip())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "breast_cancer_wisconsin.csv  full_test.txt  full_train.txt  hepatitis.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL1SSL_AeEhM"
      },
      "source": [
        "# Preprocess IMDB data\n",
        "\n",
        "import re\n",
        "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "def preprocess_reviews(reviews):\n",
        "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
        "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
        "    return reviews\n",
        "\n",
        "reviews_train_clean = preprocess_reviews(reviews_train)\n",
        "reviews_test_clean = preprocess_reviews(reviews_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw6kJceWeO_M"
      },
      "source": [
        "# Vectorizing IMDB data\n",
        "\n",
        "cv = CountVectorizer(binary=True) # Binary classification\n",
        "cv.fit(reviews_train_clean)\n",
        "imdb_train = cv.transform(reviews_train_clean)\n",
        "imdb_test = cv.transform(reviews_test_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N5YBahco2Cl"
      },
      "source": [
        "# IMDB Reviews splitting the data\n",
        "\n",
        "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
        "\n",
        "# 100% of training data\n",
        "x_train, y_train = imdb_train, target\n",
        "x_test, y_test = imdb_test, target\n",
        "\n",
        "# 20 % of training data\n",
        "x_train20 = vstack((x_train[:2500],x_train[12500:15000]), format='csr') \n",
        "target20 = [1 if i < 2500 else 0 for i in range(5000)]\n",
        "y_train20 = target20\n",
        "\n",
        "# 40 % of training data\n",
        "x_train40 = vstack((x_train[:5000],x_train[12500:17500]), format='csr')\n",
        "target40 = [1 if i < 5000 else 0 for i in range(10000)]\n",
        "y_train40 = target40\n",
        "\n",
        "# 60 % of training data\n",
        "x_train60 = vstack((x_train[:7500],x_train[12500:20000]), format='csr') \n",
        "target60 = [1 if i < 7500 else 0 for i in range(15000)]\n",
        "y_train60 = target60\n",
        "\n",
        "# 80 % of training data\n",
        "x_train80 = vstack((x_train[:10000],x_train[12500:22500]), format='csr') \n",
        "target80 = [1 if i < 10000 else 0 for i in range(20000)]\n",
        "y_train80 = target80\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYgQ-oiNHO66"
      },
      "source": [
        "#Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0P79mT0dTvw"
      },
      "source": [
        "# Multinomial Naive Bayes\n",
        "# Used: https://stackoverflow.com/questions/60969884/multinomial-naive-bayes-for-python-from-scratch\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "def logsumexp(Z):                                             \n",
        "  Zmax = np.max(Z,axis=0)                     \n",
        "  log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "  return log_sum_exp\n",
        "\n",
        "class MultinomialNaiveBayes(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "  def __init__(self, alpha=0.001):\n",
        "    self.alpha = alpha \n",
        "\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    H, L = X_train.shape\n",
        "    self.classes = np.unique(y_train)\n",
        "    numberOfClasses = len(self.classes)\n",
        "\n",
        "    self.priors = np.zeros(numberOfClasses)\n",
        "    self.likelihoods = np.zeros((numberOfClasses, L))\n",
        "\n",
        "    for i, c in enumerate(self.classes):\n",
        "        X_train_c = X_train[c == y_train]\n",
        "        self.priors[i] = X_train_c.shape[0] / H \n",
        "        self.likelihoods[i, :] = ((X_train_c.sum(axis=0)) + self.alpha) / (np.sum(X_train_c.sum(axis=0) + self.alpha))\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    return [self.helperPredict(x_test) for x_test in X_test]\n",
        "\n",
        "  def helperPredict(self, x_test):\n",
        "    posteriors = []\n",
        "    for i, c in enumerate(self.classes):\n",
        "        logprior = np.log(self.priors[i])\n",
        "        loglikelihood = self.likelihoodCalculator(self.likelihoods[i,:], x_test)\n",
        "        sumposteriors = np.sum(loglikelihood) + logprior\n",
        "        posteriors.append(sumposteriors)\n",
        "    sumexp = np.exp(posteriors - logsumexp(posteriors))\n",
        "    return self.classes[np.argmax(posteriors)]\n",
        "\n",
        "  def likelihoodCalculator(self, loglike, x_test):\n",
        "    A = np.log(loglike)\n",
        "    sA = sparse.csr_matrix(A) \n",
        "    return sA.multiply(x_test)\n",
        "\n",
        "  def score(self, X_test, y_test):\n",
        "    y_pred = self.predict(X_test)\n",
        "    count = 0;\n",
        "    for i in range(len(y_test)):\n",
        "      if y_pred[i]==y_test[i]:\n",
        "        count = count + 1\n",
        "    accuracy = count/len(y_test)\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHyNSUWQJof7"
      },
      "source": [
        "# Multinomial Naive Bayes Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfIe5Jv8GFFw"
      },
      "source": [
        "# Experiment for IMDB Reviews with optimal alpha\n",
        "\n",
        "model = MultinomialNaiveBayes(alpha=1.0)\n",
        "model.fit(x_train20,y_train20)\n",
        "accuracy = model.score(x_test,y_test)\n",
        "print(f'Test accuracy for 20% training data: {accuracy}')\n",
        "\n",
        "model.fit(x_train40,y_train40)\n",
        "accuracy = model.score(x_test,y_test)\n",
        "print(f'Test accuracy for 40% training data: {accuracy}')\n",
        "\n",
        "model.fit(x_train60,y_train60)\n",
        "accuracy = model.score(x_test,y_test)\n",
        "print(f'Test accuracy for 60% training data: {accuracy}')\n",
        "\n",
        "model.fit(x_train80,y_train80)\n",
        "accuracy = model.score(x_test,y_test)\n",
        "print(f'Test accuracy for 80% training data: {accuracy}')\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "accuracy = model.score(x_test,y_test)\n",
        "print(f'Test accuracy for 100% training data: {accuracy}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7UuHhoC3Sg5"
      },
      "source": [
        "# Experiment for twenty news group data with optimal alpha\n",
        "\n",
        "model = MultinomialNaiveBayes(alpha = 0.011)\n",
        "model.fit(X_train20,Y_train20)\n",
        "accuracy = model.score(X_test_counts,Y_test_counts)\n",
        "print(f'Test accuracy for 20% training data: {accuracy}') \n",
        "\n",
        "model.fit(X_train40,Y_train40)\n",
        "accuracy = model.score(X_test_counts,Y_test_counts)\n",
        "print(f'Test accuracy for 40% training data: {accuracy}') \n",
        "\n",
        "model.fit(X_train60,Y_train60)\n",
        "accuracy = model.score(X_test_counts,Y_test_counts)\n",
        "print(f'Test accuracy for 60% training data: {accuracy}') \n",
        "\n",
        "model.fit(X_train80,Y_train80)\n",
        "accuracy = model.score(X_test_counts,Y_test_counts)\n",
        "print(f'Test accuracy for 80% training data: {accuracy}') \n",
        "\n",
        "model.fit(X_train_counts,Y_train_counts)\n",
        "accuracy = model.score(X_test_counts,Y_test_counts)\n",
        "print(f'Test accuracy for 100% training data: {accuracy}') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtMqMYrbKOtH"
      },
      "source": [
        "# Logistic Regression Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnE8w5XBNdB1"
      },
      "source": [
        "# Logistic regression IMDB Reviews\n",
        "\n",
        "\n",
        "# Logistic regression with 100% of training data\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "   logistic_reg = LogisticRegression(C=c, max_iter=1000)\n",
        "   logistic_reg.fit(x_train, y_train)\n",
        "   print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test, logistic_reg.predict(x_test))))\n",
        "\n",
        "# Logistic regression with 80% of training data\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        " logistic_reg = LogisticRegression(C=c, max_iter=1000)\n",
        " logistic_reg.fit(x_train80, y_train80)\n",
        " print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test, logistic_reg.predict(x_test))))\n",
        "\n",
        "# Logistic regression with 60% of training data\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "   logistic_reg = LogisticRegression(C=c, max_iter=1000)\n",
        "   logistic_reg.fit(x_train60, y_train60)\n",
        "   print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test, logistic_reg.predict(x_test))))\n",
        "\n",
        "# Logistic regression with 40% of training data\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "   logistic_reg = LogisticRegression(C=c, max_iter=1000)\n",
        "   logistic_reg.fit(x_train40, y_train40)\n",
        "   print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test, logistic_reg.predict(x_test))))\n",
        "\n",
        "# Logistic regression with 20% of training data\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "   logistic_reg = LogisticRegression(C=c, max_iter=1000)\n",
        "   logistic_reg.fit(x_train20, y_train20)\n",
        "   print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test, logistic_reg.predict(x_test))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eI9qpk1KNgmR"
      },
      "source": [
        "# Logistic regression twenty news group data\n",
        "\n",
        "\n",
        "#Logistic regression with 100% of training data\n",
        "for c in [0.18,0.20,0.15,0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    logistic_reg = LogisticRegression(C=c, max_iter=1000)\n",
        "    logistic_reg.fit(X_train_counts, Y_train_counts)\n",
        "    print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(Y_test_counts, logistic_reg.predict(X_test_counts))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttD8IIqPHtez"
      },
      "source": [
        "# Logistic Regression from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj1nJk3iIGb1"
      },
      "source": [
        "# Logistic Regression from scratch\n",
        "\n",
        "import numpy as np\n",
        "#%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logistic = lambda z: 1./ (1 + np.exp(-z))       #logistic function\n",
        "\n",
        "def gradient(self, x, y):\n",
        "    N,D = x.shape\n",
        "    yh = logistic(np.dot(x, self.w))   \n",
        "    grad = np.dot(x.T, yh - y)/N       \n",
        "    return grad   \n",
        "\n",
        "def cost_fn(x, y, w):\n",
        "    N, D = x.shape                                                       \n",
        "    z = np.dot(x, w)\n",
        "    J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z))) \n",
        "    return J                     \n",
        "\n",
        "class ScratchLogisticRegression:\n",
        "\n",
        "  def __init__(self, add_bias=True, learning_rate=0.1, epsilon=1e-4, max_iters=1e5, verbose=False):\n",
        "    self.add_bias = add_bias\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epsilon = epsilon\n",
        "    self.max_iters = max_iters\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def fit(self, x, y):\n",
        "    if x.ndim ==1:\n",
        "      x = x[:, None]\n",
        "    if self.add_bias:\n",
        "      N = x.shape[0]\n",
        "      x = np.vstack([x, np.ones(N)])\n",
        "    N,D = x.shape\n",
        "    self.w = np.zeros(D)\n",
        "    g = np.inf\n",
        "    t = 0\n",
        "    # gradient descent\n",
        "    while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "      g = self.gradient(x, y)\n",
        "      self.w = self.w - self.learning_rate * g\n",
        "      t += 1\n",
        "    \n",
        "    if self.verbose:\n",
        "      print(f'Terminated after {t} iterations, norm of gradient: {np.linalg.norm(g)}')\n",
        "      print(f'weight found: {self.w}')\n",
        "    \n",
        "    return self\n",
        "\n",
        "  def predict(self, x):\n",
        "    if x.ndim==1:\n",
        "      x = x[:, None]\n",
        "    Nt = x.shape[0]\n",
        "    if self.add_bias:\n",
        "      x = np.column_stack([x, np.ones(Nt)])\n",
        "    yh = logistic(np.dot(xs,self.w))\n",
        "    return yh\n",
        "\n",
        "ScratchLogisticRegression.gradient = gradient\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjjtyU3eQ563"
      },
      "source": [
        "# comparing different learning rates for Logistic Regression\n",
        "# this is our implementation, but we were unable to run it due to running out of RAM errors\n",
        "\n",
        "def L2_loss(y_pred, y_actual):\n",
        "  return (1/2)((y_actual - y_pred)**2)\n",
        "\n",
        "def fit_and_run(model, x_train, y_train, x_test, y_test):\n",
        "  model.fit(x_train, y_train)\n",
        "  y_pred = model.predict(x_test)\n",
        "  loss = L2_loss(y_pred, y_test)\n",
        "  return loss\n",
        "\n",
        "lr = 0.001\n",
        "model = ScratchLogisticRegression(add_bias=False, learning_rate=lr)\n",
        "loss = fit_and_run(model, X_train_counts, Y_train_counts, X_test_counts, Y_test_counts)\n",
        "print(f'Loss for learning rate {lr}')\n",
        "\n",
        "lr = 0.01\n",
        "model = ScratchLogisticRegression(add_bias=False, learning_rate=lr)\n",
        "loss = fit_and_run(model, X_train_counts, Y_train_counts, X_test_counts, Y_test_counts)\n",
        "print(f'Loss for learning rate {lr}')\n",
        "\n",
        "lr = 0.1\n",
        "model = ScratchLogisticRegression(add_bias=False, learning_rate=lr)\n",
        "loss = fit_and_run(model, X_train_counts, Y_train_counts, X_test_counts, Y_test_counts)\n",
        "print(f'Loss for learning rate {lr}')\n",
        "\n",
        "lr = 1.0\n",
        "model = ScratchLogisticRegression(add_bias=False, learning_rate=lr)\n",
        "loss = fit_and_run(model, X_train_counts, Y_train_counts, X_test_counts, Y_test_counts)\n",
        "print(f'Loss for learning rate {lr}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRNYCdm5H5UX"
      },
      "source": [
        "# Linear Regression Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vIbIdGcLboE"
      },
      "source": [
        "# Linear Regression for imdb data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def runLinear(x, y):\n",
        "  model = LinearRegression()\n",
        "  model.fit(x,y)\n",
        "  y_pred = model.predict(x_test)\n",
        "  score = model.score(x_test, y_test)\n",
        "  print(f'score: {score}')\n",
        "  print(f'mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
        "\n",
        "print('Linear regression for 20% imdb training data')\n",
        "x, y = x_train20, y_train20\n",
        "runLinear(x, y)\n",
        "\n",
        "print('Linear regression for 40% imdb training data')\n",
        "x, y = x_train40, y_train40\n",
        "runLinear(x, y)\n",
        "\n",
        "print('Linear regression for 60% imdb training data')\n",
        "x, y = x_train60, y_train60\n",
        "runLinear(x, y)\n",
        "\n",
        "print('Linear regression for 80% imdb training data')\n",
        "x, y = x_train80, y_train80\n",
        "runLinear(x, y)\n",
        "\n",
        "print('Linear regression for 100% imdb training data')\n",
        "x, y = x_train, y_train\n",
        "runLinear(x, y)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87-L7QoNIMRE"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D1MRewCX8iQm"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, vstack\n",
        "# split the data for cross validation\n",
        "def cross_validation_split(x_data: csr_matrix, y_data, num_folds):\n",
        "  (num_instances, num_features) = x_data.get_shape()\n",
        "  n_val = num_instances // num_folds # num of data samples in each split\n",
        "  # print(n_val)\n",
        "  inds = np.random.permutation(num_instances) # shuffle \n",
        "  folds_x = [None] * num_folds\n",
        "  folds_y = [None] * num_folds\n",
        "  for f in range(num_folds):\n",
        "    print(f'Creating fold {f}')\n",
        "    r1 = int(f*n_val)\n",
        "    r2 = int((f+1)*n_val)\n",
        "    folds_x[f] = x_data.getrow(inds[r1])\n",
        "    for i in range(r1+1,r2):\n",
        "      row = x_data.getrow(inds[i])\n",
        "      folds_x[f] = vstack([folds_x[f], row]) # add row to folds\n",
        "    folds_x[f].tocsr\n",
        "    folds_y[f] = (y_data[inds[r1 : r2]])\n",
        "  return folds_x, np.array(folds_y)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ORvBWtMlf4EM"
      },
      "source": [
        "#define a function for the MSE loss\n",
        "loss = lambda y, yh: np.mean((y-yh)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CFCJSVgG_3eE"
      },
      "source": [
        "# Cross Validation kfoldCV\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, vstack\n",
        "\n",
        "\n",
        "\n",
        "def kfoldCV(model, folds_x, folds_y):\n",
        "  num_folds = len(folds_x)\n",
        "  print(f'Num folds: {num_folds}')\n",
        "  (rows_in_fold, cols_in_fold) = folds_x[0].get_shape()\n",
        "  acc = 0 # holds sum of accuracies\n",
        "  # go through each fold and run\n",
        "  err = []\n",
        "  for f in range(num_folds):\n",
        "    # get validation set\n",
        "    validation_set_x = folds_x[f]\n",
        "    validation_set_y = folds_y[f]\n",
        "    # assemble training set from the other folds\n",
        "    training_set_x = csr_matrix(((num_folds-1)*rows_in_fold, cols_in_fold))\n",
        "    training_set_y = [None] * (num_folds-1)*rows_in_fold \n",
        "    below = csr_matrix(((max(0,f-1))*rows_in_fold, cols_in_fold))\n",
        "    above = csr_matrix((max((0, num_folds-(f+1)))*rows_in_fold, cols_in_fold))\n",
        "    if f > 0 :\n",
        "      accum1 = folds_x[0]\n",
        "      for i in range(1, f-1):\n",
        "        accum1 = vstack([accum1, folds_x[i]])\n",
        "      below = accum1\n",
        "\n",
        "    if f < num_folds-1:\n",
        "      accum2 = folds_x[f+1]\n",
        "      for i in range(f+2, num_folds):\n",
        "        accum2 = vstack([accum2, folds_x[i]])\n",
        "      above = accum2;\n",
        "    \n",
        "    if f == 0:\n",
        "      training_set_y = folds_y[f+1:].flatten()\n",
        "    elif f >0 and f < num_folds -1:\n",
        "      first = folds_y[:f-1].flatten()\n",
        "      second = folds_y[f+1:].flatten()\n",
        "      training_set_y = np.concatenate((first, second), axis=None)\n",
        "    else:\n",
        "      training_set_y = folds_y[:f-1].flatten()\n",
        "    \n",
        "    # create training set\n",
        "    training_set_x = vstack([below, above])\n",
        "    print(f'Running on fold {f}')\n",
        "    #then run model\n",
        "    model.fit(training_set_x, training_set_y)\n",
        "    #get accuracy\n",
        "\n",
        "    pred = model.predict(validation_set_x)\n",
        "    f_loss = loss(validation_set_y, pred)\n",
        "    err.append(f_loss)\n",
        "    print(f'Fold {f} loss: {f_loss}')\n",
        "\n",
        "  return err"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QdqXU4xuyl0r"
      },
      "source": [
        "#actually do the cross validation\n",
        "from sklearn.utils.fixes import loguniform\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cross_validate(x_train, y_train, x_test, y_test):\n",
        "\n",
        "  (num_instances, num_features) = x_train.get_shape()\n",
        "  print(f'num instances: {num_instances}, num_features: {num_features}')\n",
        "\n",
        "  # get folds for 5-fold cross validation\n",
        "  folds_x, folds_y = cross_validation_split(x_train, y_train, 5)\n",
        "\n",
        "  # this is where we try different hyperparams\n",
        "  alpha_list =[0.0001, 0.001, 0.01, 0.1, 1.0]\n",
        "  accuracies = []\n",
        "  err_test, err_valid = np.zeros(len(alpha_list)), np.zeros((len(alpha_list), 5))\n",
        "  i = 0\n",
        "  for a in alpha_list:\n",
        "    print(f'alpha: {a}')\n",
        "    model = MultinomialNaiveBayes(alpha = a)\n",
        "    err_valid[i] = kfoldCV(model, folds_x, folds_y)\n",
        "    #test error\n",
        "    model.fit(x_train, y_train)\n",
        "    err_test[i] = loss(y_test, model.predict(x_test))\n",
        "    i += 1\n",
        "\n",
        "  plt.plot(alpha_list, err_test, label='test')\n",
        "  plt.plot(alpha_list, err_valid, axis = 1, label='validation')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.xlabel('alpha')\n",
        "  plt.ylabel('mean squared error')\n",
        "  plt.show()\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm6zSilk_Q4C"
      },
      "source": [
        "# Cross Validation from scratch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# twenty news group data\n",
        "train_x = X_train_counts\n",
        "train_y = Y_train_counts\n",
        "\n",
        "test_x = X_test_counts\n",
        "test_y = Y_test_counts\n",
        "\n",
        "cross_validate(train_x, train_y, test_x, test_y)\n",
        "\n",
        "# imdb data\n",
        "train_x = x_train\n",
        "train_y = np.array(y_train)\n",
        "\n",
        "test_x = x_test\n",
        "test_y = np.array(y_test)\n",
        "\n",
        "cross_validate(train_x, train_y, test_x, test_y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiYgoK13tAAI"
      },
      "source": [
        "#cross validation using sklearn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "import scipy.stats as stats\n",
        "from sklearn.utils.fixes import loguniform\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# 20 data\n",
        "X, y = X_train_counts, Y_train_counts\n",
        "\n",
        "# build a classifier\n",
        "clf = MultinomialNaiveBayes()\n",
        "\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
        "                  .format(results['mean_test_score'][candidate],\n",
        "                          results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 4\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, n_jobs = -1, cv=5)\n",
        "print(f'Running CV on 20 data set')\n",
        "start = time()\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n",
        "# imdb data\n",
        "X, y = x_train, y_train\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 4\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, cv=5)\n",
        "print(f'Running CV on imdb data set')\n",
        "start = time()\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP1aE6PjWo7_"
      },
      "source": [
        "# logistic regression cross validation\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {'C': [0.001, 0.01, 0.1, 0.25, 0.5, 1.0]}\n",
        "\n",
        "# run randomized search\n",
        "clf = LogisticRegression(max_iter = 1000)\n",
        "n_iter_search = 6\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, n_jobs = -1, cv=5)\n",
        "\n",
        "X, y = X_train_counts, Y_train_counts\n",
        "print(f'Running CV on 20 data set for logistic regression')\n",
        "start = time()\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n",
        "X, y = x_train, y_train\n",
        "print(f'Running CV on imdb data set for logistic regression')\n",
        "start = time()\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3B_d6TNKE4p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}